{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers faiss-cpu torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3ff921487d43a8aeaf94c5aa690274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a803f0fd6be476d97c8e38823b86e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb92b20cd0aa4d5c987e28a2d52bd586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144097fcf685435aa83e5798e4530614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc617a79394c47bc6ac8fbe16e0b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d7f41ad44c4429a9d85b846b6977f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafbb6c21ddb45cbb4f6e5708184757d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d330fb55a404196bdf3fed0d647a32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/6.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29db31680b5b4bdc9ef46712e877cc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/94.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c2e3b2c2c34dee94ddc343acd0683f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/8.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652a080442554bfa86c3ffadfa386acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4dbbe9e28f4996a46e661f32b96cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba64fe8df024e49965b0994f45bafea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d067b684824caeb45587450a1254e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e2b576d71f4733b2949fe50b9b0289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3caced918d4f6fbf8aaa83eba8696a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f37e08e8f8a46a8b5b1e95c47af1381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5ab4cde3864c22b5d6597de5c51e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f0b087a88a446291ade4618e9aa437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7088cc56858e46aebce960355c032c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf6b7b0554b4c90a20650fc4f29bbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed5db0bc65947b3b20481e5f309387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64677705ff1241d38f154a724b321541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5bc7664b314c8ca73aff8c53d717bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Llama-2 model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "403 Client Error. (Request ID: Root=1-67bfbdb4-2d67e9cf4c639b4d7e447a21;345b7584-ffcc-4951-96ed-02267bd299db)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.\n",
      "Models Loaded!\n",
      "Amharic to English: enenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenen\n",
      "English to Amharic: am Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? Howen are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you? How are you?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set Hugging Face token from environment variable (if needed)\n",
    "hf_token = \"\" # Replace \"HF_TOKEN\" with your env variable name\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Translation Models (Low-resource <-> English)\n",
    "# Target low-resource language: Amharic (am)\n",
    "low_resource_lang = \"am\"  # Amharic language code\n",
    "high_resource_lang = \"en\"  # English\n",
    "\n",
    "# Load M2M100 model once for both directions (1.2B parameters)\n",
    "# translator_model_name = \"facebook/m2m100_1.2B\"  # Multilingual model supporting Amharic\n",
    "# translator_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "translator_model_name = \"facebook/nllb-200-3.3B\"  # Latest high-performance translation model\n",
    "\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translator_model_name, token=hf_token)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translator_model_name, token=hf_token)\n",
    "\n",
    "# Amharic to English translation pipeline\n",
    "translator_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"am\",  # Source language: Amharic\n",
    "    tgt_lang=\"en\",  # Target language: English\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# English to Amharic translation pipeline (reusing the same model)\n",
    "translator_from_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"en\",  # Source language: English\n",
    "    tgt_lang=\"am\",  # Target language: Amharic\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# 2. Sentence Embeddings Model (for RAG)\n",
    "embed_model_name = \"all-mpnet-base-v2\"  # Good general-purpose sentence embeddings\n",
    "embed_model = SentenceTransformer(embed_model_name, device=device)  # Use GPU if available\n",
    "\n",
    "# 3. Generative AI Model (GenAI) - Llama 2 (Requires Hugging Face Token and Access)\n",
    "gen_ai_model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Llama 2 7B Chat\n",
    "\n",
    "try:\n",
    "    gen_ai_tokenizer = AutoTokenizer.from_pretrained(gen_ai_model_name, token=hf_token)\n",
    "    gen_ai_model = AutoModelForCausalLM.from_pretrained(\n",
    "        gen_ai_model_name,\n",
    "        torch_dtype=torch.float16,  # Half precision for memory efficiency\n",
    "        device_map=\"auto\",  # Auto-distribute across GPU(s)\n",
    "        token=hf_token\n",
    "    )\n",
    "    gen_ai_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=gen_ai_model,\n",
    "        tokenizer=gen_ai_tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Llama-2 model: {e}\")\n",
    "    gen_ai_pipeline = None  # Fallback if Llama-2 fails to load\n",
    "\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# Optional: Test the translation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Amharic to English\n",
    "    text_am = \"ሰላም አለም\"  # \"Hello World\" in Amharic\n",
    "    result_to_en = translator_to_en(text_am)\n",
    "    print(f\"Amharic to English: {result_to_en[0]['translation_text']}\")\n",
    "\n",
    "    # Test English to Amharic\n",
    "    text_en = \"How are you?\"\n",
    "    result_to_am = translator_from_en(text_en)\n",
    "    print(f\"English to Amharic: {result_to_am[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881279fbc41e4c2fafe7b53a276103f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4706aeda02346d9887f061b2ffab78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/909 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed2687d7855458d8c31c27da376c918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b8c37f4a8a423a9fd472ec698065ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dd305a1e514f1ba0ad9fa7e3897bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6259d521860f47c68d448d823d110f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c5e1326a604adaa82fccc2d530629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffd78850f4b49fc94bdb29171e5f018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Llama-2 model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "403 Client Error. (Request ID: Root=1-67c01df1-6025cddc10d4b82b16267800;b85edc87-5cbe-484c-82fa-e37d7642fe01)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.\n",
      "Models Loaded!\n",
      "Amharic to English: Hello world\n",
      "English to Amharic: አንተ እንዴት ነው?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set Hugging Face token from environment variable (if needed)\n",
    "hf_token = \"\" # Replace \"HF_TOKEN\" with your env variable name\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Translation Models (Low-resource <-> English)\n",
    "# Target low-resource language: Amharic (am)\n",
    "low_resource_lang = \"am\"  # Amharic language code\n",
    "high_resource_lang = \"en\"  # English\n",
    "\n",
    "# Load M2M100 model once for both directions (1.2B parameters)\n",
    "translator_model_name = \"facebook/m2m100_1.2B\"  # Multilingual model supporting Amharic\n",
    "# translator_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "# translator_model_name = \"facebook/nllb-200-3.3B\"  # Latest high-performance translation model\n",
    "\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translator_model_name, token=hf_token)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translator_model_name, token=hf_token)\n",
    "\n",
    "# Amharic to English translation pipeline\n",
    "translator_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"am\",  # Source language: Amharic\n",
    "    tgt_lang=\"en\",  # Target language: English\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# English to Amharic translation pipeline (reusing the same model)\n",
    "translator_from_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"en\",  # Source language: English\n",
    "    tgt_lang=\"am\",  # Target language: Amharic\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# 2. Sentence Embeddings Model (for RAG)\n",
    "embed_model_name = \"all-mpnet-base-v2\"  # Good general-purpose sentence embeddings\n",
    "embed_model = SentenceTransformer(embed_model_name, device=device)  # Use GPU if available\n",
    "\n",
    "# 3. Generative AI Model (GenAI) - Llama 2 (Requires Hugging Face Token and Access)\n",
    "gen_ai_model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Llama 2 7B Chat\n",
    "\n",
    "try:\n",
    "    gen_ai_tokenizer = AutoTokenizer.from_pretrained(gen_ai_model_name, token=hf_token)\n",
    "    gen_ai_model = AutoModelForCausalLM.from_pretrained(\n",
    "        gen_ai_model_name,\n",
    "        torch_dtype=torch.float16,  # Half precision for memory efficiency\n",
    "        device_map=\"auto\",  # Auto-distribute across GPU(s)\n",
    "        token=hf_token\n",
    "    )\n",
    "    gen_ai_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=gen_ai_model,\n",
    "        tokenizer=gen_ai_tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Llama-2 model: {e}\")\n",
    "    gen_ai_pipeline = None  # Fallback if Llama-2 fails to load\n",
    "\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# Optional: Test the translation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Amharic to English\n",
    "    text_am = \"ሰላም አለም\"  # \"Hello World\" in Amharic\n",
    "    result_to_en = translator_to_en(text_am)\n",
    "    print(f\"Amharic to English: {result_to_en[0]['translation_text']}\")\n",
    "\n",
    "    # Test English to Amharic\n",
    "    text_en = \"How are you?\"\n",
    "    result_to_am = translator_from_en(text_en)\n",
    "    print(f\"English to Amharic: {result_to_am[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to Amharic: amasa: Liverpool drew 2-0 at Newcastle on Wednesday night, while Arsenal were held to a goalless draw at Nottingham Forest. No team has ever lost the Premier League title by such a margin with 10 games remaining. The Reds have only lost once this season. The fact that title rivals Arsenal have taken only one point from their last two games has helped Liverpool's progress. Arsenal manager Mikel Arteta, for his part, said before the Nottingham game that he absolutely did not accept the idea that the trophy had been taken from Arsenal, adding that after the game it was still the same as a week or two ago. all we can do is win games, he told the BBC.\n"
     ]
    }
   ],
   "source": [
    "text_e = \"Liverpool extended their lead over Arsenal to 13 points with a midweek Premier League win. Liverpool drew 2-0 at Newcastle on Wednesday night, while Arsenal were held to a goalless draw at Nottingham Forest. No team has ever lost the Premier League title by such a margin with 10 games remaining. The Reds have only lost once this season. The fact that title rivals Arsenal have only taken one point from their last two games has helped Liverpool's progress. Arsenal manager Mikel Arteta, for his part, said before the Nottingham game that he absolutely did not accept the idea that the trophy had been taken from Arsenal, adding that after the game it was still the same as a week or two ago. All we can do is win games, he told the BBC.\"\n",
    "result_to_am = translator_from_en(text_e)\n",
    "print(f\"English to Amharic: {result_to_am[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 312 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to Amharic: enredredreds have lost only once in the past season. the rivalry rivalry has only lost one point in the last two games. if the rivalry rivalry has lost only one point in the last two games, we can still make a suggestion to Liverpool. we have still made suggestions to coach the team. we have still made suggestions to play before the match.\n"
     ]
    }
   ],
   "source": [
    "text_e = \"በሳምንቱ አጋማሽ የፕሪሚዬር ሊግ ጨዋታ ድል የቀናው ሊቨርፑል ከአርሰናል ጋር ያለውን የነጥብ ልዩነት ወደ 13 ማስፋት ችሏል። ረቡዕ ምሽት በሜዳው አንፊልድ ኒውካስልን ያስተናገደው ሊቨርፑል ሁለት ለምንም ሲረታ ወደ ኖቲንግሀም ሜዳ ያቀናው አርሰናል ደግሞ ያለጎል ተለያይቷል። በእንግሊዝ ፕሪሚዬር ሊግ ታሪክ ውድድሩ 10 ጨዋታዎች እየቀሩት በዚህ የነጥብ ልዩነት እየመራ ዋንጫውን ያጣ ክለብ የለም። ቀያዮቹ በያዝነው የውድድር ዘመን አንድ ጊዜ ብቻ ነው ሽንፈት የቀመሱት። የዋንጫ ተፎካካሪው አርሰናል ካለፉት ሁለት ጨዋታዎች አንድ ነጥብ ብቻ ማግኘቱ ለሊቨርፑል ግስጋሴ አስተዋፅዖ አድርጓል። የሊቨርፑሉ አሰልጣኝ አርን ስሎት አሁንም ገና ብዙ ይቀረናል። 10 ጨዋታዎች አሉ ሲሉ አስተያየታቸውን ሰጥተዋል። የአርሰናሉ አሰልጣኝ ሚኬል አርቴታ በበኩላቸው ከኖቲንግሀም ጨዋታ በፊት በሰጡት አስተያየት ዋንጫው ከአርሰናል እጅ ወጥቷል የሚለውን ሐሳብ በፍፁም አልቀበለውም ብለዋል። ከጨዋታው በኋላ ደግሞ አሁንም ከሳምንት ከሁለት ሳምንት በፊት ተመሳሳይ ነው። እኛ ማድረግ የምንችለው ጨዋታዎችን ማሸነፍ ነው ሲሉ ለቢቢሲ ተናግረዋል።\"\n",
    "result_to_en = translator_to_en(text_e)\n",
    "print(f\"English to Amharic: {result_to_en[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda08c5171d54ce09f3950f6358f65bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7920a7f7df40a8b9a1d1e1028c7994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae18637532744501a860cc05d4886670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104cfeb6f4b944a09b430352be7e121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e5657a6bc14920bffffc331a9fd425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c071a5a459f4f76b3382b17de0869a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f86af09e6c4a94b2a8cdfe8642a6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf03d57b244dac8aaa254835451d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Llama-2 model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "403 Client Error. (Request ID: Root=1-67c04cd7-58267822318d0b021f83760a;25612ea7-cfaa-47f9-90e5-6d50631da222)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.\n",
      "Models Loaded!\n",
      "Amharic to English: en peace world am\n",
      "English to Amharic: am, how are you?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set Hugging Face token from environment variable (if needed)\n",
    "hf_token = \"\" # Replace \"HF_TOKEN\" with your env variable name\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Translation Models (Low-resource <-> English)\n",
    "# Target low-resource language: Amharic (am)\n",
    "low_resource_lang = \"am\"  # Amharic language code\n",
    "high_resource_lang = \"en\"  # English\n",
    "\n",
    "# Load M2M100 model once for both directions (1.2B parameters)\n",
    "# translator_model_name = \"facebook/m2m100_1.2B\"  # Multilingual model supporting Amharic\n",
    "translator_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "# translator_model_name = \"facebook/nllb-200-3.3B\"  # Latest high-performance translation model\n",
    "\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translator_model_name, token=hf_token)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translator_model_name, token=hf_token)\n",
    "\n",
    "# Amharic to English translation pipeline\n",
    "translator_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"am\",  # Source language: Amharic\n",
    "    tgt_lang=\"en\",  # Target language: English\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# English to Amharic translation pipeline (reusing the same model)\n",
    "translator_from_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=translator_model,\n",
    "    tokenizer=translator_tokenizer,\n",
    "    src_lang=\"en\",  # Source language: English\n",
    "    tgt_lang=\"am\",  # Target language: Amharic\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "# 2. Sentence Embeddings Model (for RAG)\n",
    "embed_model_name = \"all-mpnet-base-v2\"  # Good general-purpose sentence embeddings\n",
    "embed_model = SentenceTransformer(embed_model_name, device=device)  # Use GPU if available\n",
    "\n",
    "# 3. Generative AI Model (GenAI) - Llama 2 (Requires Hugging Face Token and Access)\n",
    "gen_ai_model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Llama 2 7B Chat\n",
    "\n",
    "try:\n",
    "    gen_ai_tokenizer = AutoTokenizer.from_pretrained(gen_ai_model_name, token=hf_token)\n",
    "    gen_ai_model = AutoModelForCausalLM.from_pretrained(\n",
    "        gen_ai_model_name,\n",
    "        torch_dtype=torch.float16,  # Half precision for memory efficiency\n",
    "        device_map=\"auto\",  # Auto-distribute across GPU(s)\n",
    "        token=hf_token\n",
    "    )\n",
    "    gen_ai_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=gen_ai_model,\n",
    "        tokenizer=gen_ai_tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Llama-2 model: {e}\")\n",
    "    gen_ai_pipeline = None  # Fallback if Llama-2 fails to load\n",
    "\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# Optional: Test the translation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Amharic to English\n",
    "    text_am = \"ሰላም አለም\"  # \"Hello World\" in Amharic\n",
    "    result_to_en = translator_to_en(text_am)\n",
    "    print(f\"Amharic to English: {result_to_en[0]['translation_text']}\")\n",
    "\n",
    "    # Test English to Amharic\n",
    "    text_en = \"How are you?\"\n",
    "    result_to_am = translator_from_en(text_en)\n",
    "    print(f\"English to Amharic: {result_to_am[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 312 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to Amharic: am-game.com - The Red have lost only once in the past season.The Red have lost only once in the last two games.The Cup contender has lost only one point in the last two games.If we have lost only one point in the last two games, then it is still a matter of time before the team has lost the game.We have still gotten feedback from the coaches that they have lost the game.\n"
     ]
    }
   ],
   "source": [
    "text_e = \"በሳምንቱ አጋማሽ የፕሪሚዬር ሊግ ጨዋታ ድል የቀናው ሊቨርፑል ከአርሰናል ጋር ያለውን የነጥብ ልዩነት ወደ 13 ማስፋት ችሏል። ረቡዕ ምሽት በሜዳው አንፊልድ ኒውካስልን ያስተናገደው ሊቨርፑል ሁለት ለምንም ሲረታ ወደ ኖቲንግሀም ሜዳ ያቀናው አርሰናል ደግሞ ያለጎል ተለያይቷል። በእንግሊዝ ፕሪሚዬር ሊግ ታሪክ ውድድሩ 10 ጨዋታዎች እየቀሩት በዚህ የነጥብ ልዩነት እየመራ ዋንጫውን ያጣ ክለብ የለም። ቀያዮቹ በያዝነው የውድድር ዘመን አንድ ጊዜ ብቻ ነው ሽንፈት የቀመሱት። የዋንጫ ተፎካካሪው አርሰናል ካለፉት ሁለት ጨዋታዎች አንድ ነጥብ ብቻ ማግኘቱ ለሊቨርፑል ግስጋሴ አስተዋፅዖ አድርጓል። የሊቨርፑሉ አሰልጣኝ አርን ስሎት አሁንም ገና ብዙ ይቀረናል። 10 ጨዋታዎች አሉ ሲሉ አስተያየታቸውን ሰጥተዋል። የአርሰናሉ አሰልጣኝ ሚኬል አርቴታ በበኩላቸው ከኖቲንግሀም ጨዋታ በፊት በሰጡት አስተያየት ዋንጫው ከአርሰናል እጅ ወጥቷል የሚለውን ሐሳብ በፍፁም አልቀበለውም ብለዋል። ከጨዋታው በኋላ ደግሞ አሁንም ከሳምንት ከሁለት ሳምንት በፊት ተመሳሳይ ነው። እኛ ማድረግ የምንችለው ጨዋታዎችን ማሸነፍ ነው ሲሉ ለቢቢሲ ተናግረዋል።\"\n",
    "result_to_am = translator_from_en(text_e)\n",
    "print(f\"English to Amharic: {result_to_am[0]['translation_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
